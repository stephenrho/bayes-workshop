---
title: "Introduction to `brms`"
author: "Stephen Rhodes - srhodes@research.baycrest.org"
date: 'Last updated: `r Sys.Date()`'
output:
  ioslides_presentation:
    logo: images/brms.png
    widescreen: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = F, message = F)

library(brms)
library(bayesplot)

load("../examples/brms-example1.RData")
```

## Introduction

> - `brms` = Bayesian Regression Models using Stan
> - Created by [Paul-Christian BÃ¼rkner](https://github.com/paul-buerkner/brms)
> - An interface to the [Stan](https://mc-stan.org/) probabilistic programming language + sampling routines
> - Allows fitting of a huge range of models using `R`'s formula syntax
> - These slides aim to introduce the main functions

## R formula syntax

|  term  | meaning | 
| :------: | -------: |
| `y ~ 1 + x` | `y` predicted by `x` (1 = intercept, included by default) |
| `y ~ 1 + x + z + x:z` | two variables and interaction (equivalent to `y ~ x*z`) | 
| `(1 | g)` | allow the intercept to vary by levels of `g` | 
| `(1 + x | g)` | allow intercept and slope to vary and model correlation between these 'random effects' |
| `(0 + x | g)` | fixed intercept, random slope |
| `(1 + x || g)` | model random effects but not correlation (fix $\rho$ to 0) |

## `brm()`

```r
brm(formula, data, family = gaussian(), prior = NULL,
  sample_prior = c("no", "yes", "only"),
  chains = 4, iter = 2000, warmup = floor(iter/2), 
  thin = 1, cores = getOption("mc.cores", 1L))
```

see `?brm` for more

- We'll work through an example with the `sleepstudy` data set from `lme4` (in `examples/sleepstudy.csv`)
- Example code in `examples/brms-example1.R`

## sleepstudy

Data from a sleep deprivation study

Reaction times for `r length(unique(sleepstudy$Subject))` participants following 0-9 Days of sleep deprivation

```{r}
head(sleepstudy)
```

## sleepstudy

```{r, echo=F}
plot(NA, xlim=c(0,9), ylim=c(190,500), axes=F, xlab="Days", ylab="Reaction", main="The sleepstudy dataset")
axis(1); axis(2)
x=lapply(unique(sleepstudy$Subject), 
       function(x) with(subset(sleepstudy, Subject==x), lines(Days, Reaction, col='grey')))
with(aggregate(Reaction~Days, FUN = mean, data = sleepstudy), points(Days, Reaction, pch=16, type='b', col='red'))
legend(0, 500, legend = c("Individual", "Average"), col = c("grey", "red"), pch = c(NA, 16), lty=c(1,1))

```

# Setting priors

## `get_prior()`

Given a model formula you can find the parameters to set priors for via (below are the defaults used by brms):

```{r}
get_prior(Reaction ~ Days, data = sleepstudy, family = gaussian)
```

We'll start with a *complete pooling* model (not acknowledging the clustering in the data)

## `set_prior()`

```r
priors = c(set_prior("normal(500, 100)", class = "Intercept"),
           set_prior("normal(0, 50)", class = "b"), # fixed effect of 'Days'
           set_prior("cauchy(0, 50)", class = "sigma"))
```

or

```r
priors = set_prior("normal(500, 100)", class = "Intercept") +
           set_prior("normal(0, 50)", class = "b") +
           set_prior("cauchy(0, 50)", class = "sigma")
```

## `brm()` - sample from the prior distribution

```r
p1 = brm(Reaction ~ Days, data = sleepstudy, 
         family = gaussian, 
         prior = priors,
         sample_prior = "only")
```

## Prior Predictive Distribution

```{r, fig.height=3, fig.width=5}
marginal_effects(p1, method="predict")
```

- `marginal_effects()` can display the "fitted" regression line (or condition differences)
- or it can display marginal predictions, as done here for prior samples

## Prior Predictive Distribution

- The predictions of small (and negative!) reaction times are a little worrying
- We could specify truncation of the data (e.g., `Reaction | trunc(lb=0) ~ ...`)
- But this is ok for our purposes

# Fitting models

## `brm()` - fit model

```r
m1 = brm(Reaction ~ Days, data = sleepstudy, 
         family = gaussian, 
         prior = priors,
         save_all_pars = T)
```

## `brmsfit` object

```{r}
summary(m1)
```

## `brmsfit` object 

```{r}
fixef(m1) # fixed effects

VarCorr(m1) # residual SD

rhat(m1) # R hat statistic (convergence)
```

## `brmsfit` object 

```{r}
plot(m1) # plot posterior samples
```

# Posterior predictive checks and marginal effects

## `pp_check`

Compare the distribution of the data with distributions generated from the model 

```{r}
pp_check(m1, nsamples = 100)
```

## The `bayesplot` package

- The [`bayesplot` package](https://mc-stan.org/bayesplot/) has lots of functions for plotting Bayesian models (see also `brms::stanplot` function)
- Particularly useful for posterior predictive checks
- To use, we need to generate predictive distributions ($y_{rep}$)

## $y_{rep}$ via `posterior_predict`

```{r}
yrep = posterior_predict(m1)

dim(yrep) # row for each step in chain, column for each observation
```

## `bayesplot::ppc_stat`

```{r}
ppc_stat(y = sleepstudy$Reaction, yrep = yrep) # stat = "mean" by default
```

## `bayesplot::ppc_stat`

```{r}
ppc_stat(y = sleepstudy$Reaction, yrep = yrep, stat = "min")
```

## `bayesplot::ppc_stat_grouped`

```{r, fig.height=4, fig.width=8}
ppc_stat_grouped(y = sleepstudy$Reaction, yrep = yrep, 
                 group = sleepstudy$Subject)
```

## `marginal_effects`

We can plot the fitted regression line

```{r}
marg_eff1 = marginal_effects(m1)
plot(marg_eff1)
```

## `marginal_effects`

`marginal_effects` also returns a data frame with median (estimate__) and 95% credible intervals (lower__, upper__) for different predictor values

```{r}
head(marg_eff1$Days)
```

# Models with random effects | Introducing partial pooling

## Model 2: random intercept

```r
# need to add a prior for the random participant effect
priors = c(priors,
           set_prior("cauchy(0, 100)", class = "sd")) # SD of random intercept

m2 = brm(Reaction ~ Days + (1 | Subject), 
         data = sleepstudy, family = gaussian, 
         prior=priors)
```

$$
y_i \sim \mbox{Normal}(\beta_0 + b_{j[i]} + \beta_1x_i, \; \sigma)
$$
$$
b_j \sim \mbox{Normal}(0, \sigma^{(b)})
$$

## summary (note: Group-Level Effects:)

```{r}
summary(m2)
```

## Model 2

```{r}
fixef(m2) # fixed effects

VarCorr(m2) # Subject intercept and residual SDs
```

## more `marginal_effects`

```{r}
marginal_effects(m2)
```

## more `marginal_effects`

Plot marginal effects for participants 1 to 6. `re_formula = NULL` includes random effects (`NA` excludes them, default)

```{r, fig.height=3, fig.width=5}
plot(marginal_effects(m2, 
                      conditions = data.frame(Subject=unique(sleepstudy$Subject)[1:6]), 
                      re_formula = NULL), points=T)
```

## Plot random effects

```{r}
stanplot(m2, pars = 'r_Subject') # see next slide on how to find pars
```

## `parnames`

If you don't know *all* the names of the parameters in your model...

```{r}
parnames(m2)
```

## m3: random intercept + slope

```r
priors = c(priors,
           set_prior("lkj(1)", class = "cor")) # uniform prior on correlation
           
m3 = brm(Reaction ~ Days + (1 + Days | Subject), 
         data = sleepstudy, family = gaussian, 
         prior=priors) 
```

$$
y_i \sim \mbox{Normal}(\beta_0 + b_{0j[i]} + (\beta_1 + b_{1j[i]})x_i, \; \sigma)
$$

$$
\begin{pmatrix}
b_{0j} \\
b_{1j}
\end{pmatrix} \sim MVN\left(\begin{pmatrix}
0 \\
0
\end{pmatrix}, \; \Sigma
\right).
$$

See [here](https://mc-stan.org/docs/2_18/stan-users-guide/multivariate-hierarchical-priors-section.html) for more detail on multivariate priors

## Model 3: random intercept + slope

```{r}
summary(m3)
```

## Model 3: random intercept + slope

```{r}
VarCorr(m3)$Subject$sd # random effects SDs

VarCorr(m3)$Subject$cor # random effects correlation
```

## Model 3: random intercept + slope

```{r, fig.height=3, fig.width=5}
plot(marginal_effects(m3, 
                      conditions = data.frame(Subject=unique(sleepstudy$Subject)[1:6]), 
                      re_formula = NULL), points=T)
```

# Comparing models

## `add_criterion`

- Add model fit criteria to the model objects
- Can add
    - approximate loo (via psis)
    - waic
    - k-fold cross validation
    - $R^2$ (more [here](https://www.tandfonline.com/doi/abs/10.1080/00031305.2018.1549100))
    - marginal likelihood (via bridge sampling)

## `add_criterion`

```r
m1 = add_criterion(m1, c("loo", "waic", "marglik"))
m2 = add_criterion(m2, c("loo", "waic", "marglik"))
m3 = add_criterion(m3, c("loo", "waic", "marglik"))
```

```{r}
m1$loo
```

## `loo_compare`

```r
m1 = add_criterion(m1, c("loo", "waic", "marglik"))
m2 = add_criterion(m2, c("loo", "waic", "marglik"))
m3 = add_criterion(m3, c("loo", "waic", "marglik"))
```

```{r}
loo_compare(m1, m2, m3) # or loo_compare(m1,m2,m3,criterion="waic")
```

- `abs(elpd_diff)` > 2*`se_diff` favors top model (more on loo [here](https://cran.r-project.org/web/packages/loo/vignettes/loo2-example.html#comparing-the-models-on-expected-log-predictive-density))

## Bayes factors

- The `marglik` criterion is calculated by the [`bridgesampling` package](https://cran.r-project.org/web/packages/bridgesampling/index.html)
- The functions are loaded by `brms`
- To calculate `marglik` the argument `save_all_pars = T` must be set in `brm()`

## Bayes factors

```{r}
bayes_factor(m2, m1)
bayes_factor(m3, m2)
```

```{r}
post_prob(m1, m2, m3, prior_prob = c(1/3, 1/3, 1/3))
```

$$
\frac{p(M_1 \mid y)}{p(M_2 \mid y)} = B_{1,2} \times \frac{p(M_1)}{p(M_2)}
$$

## To calculate each criterion separately

- `loo()`
- `waic()`
- `kfold()`
- `bayes_R2()`
- `bridge_sampler()`

# Do things with posterior samples

## `hypothesis`

- Savage-Dickey density ratio (requires `sample_prior = T or "yes"` in `brm()`)

```{r}
(m3_sav = hypothesis(x = m3, hypothesis = "Days = 0"))
```

## `hypothesis`

- The posterior doesn't overlap zero (which gives `Evid.Ratio = 0`)
- There are better examples in the other example scripts...

```{r, fig.height=3, fig.width=6}
plot(m3_sav)
```

## Extract samples - `posterior_samples`

```{r}
m3_days = posterior_samples(m3, pars = "b_Days")[,1]
```

```{r, echo=F}
hist(m3_days, breaks = 30, border = F, col="lightblue", main="Change in Reaction per Day (m3)", xlab="", ylab="", axes=F)
axis(1)
lines(x = quantile(m3_days, probs = c(.025, .975)), y=c(0,0), lwd=3)
text(mean(m3_days), 60, labels = sprintf("Mean + 95%% CI:\n%.2f [%.2f, %.2f]",
                                         mean(m3_days), 
                                         quantile(m3_days, probs = c(.025)), 
                                         quantile(m3_days, probs = c(.975))))
```


## Use samples to make predictions - `posterior_linpred`

- Alternative to `marginal_effects` (and more flexible)
- Below we make predictions for a new person (a new level of 'Subject')
- Predictions are for *expected value* (i.e., 'regression line'; use `posterior_predict` to include residual variance)

```{r}
newdat = data.frame(Days = 0:9, Subject=100)
m3_newp_pp = posterior_linpred(m3, # the fitted() function could also work
                               re_formula = NULL, # default, include random effects
                               newdata = newdat, 
                               allow_new_levels=T)
```

## Use samples to make predictions

Each column represents a row in `newdat` (see previous slide)

```{r}
head(m3_newp_pp)
```

## Use samples to make predictions

Predictions for a new 'Subject' (based on 1000 samples from the posterior)

```{r, echo=F}
# plot 1000 posterior predictions
plot(NA, xlim=c(1,10), ylim=c(100,500), axes=F, xlab="Days", ylab="Reaction", main="posterior predictions for new person")
axis(1, at=1:10, labels = 0:9); axis(2)
invisible(lapply(sample(4000, size = 1000), 
       function(x) lines(m3_newp_pp[x,], col=rgb(.2,.2,.2,.2))))
```

## Use samples to make predictions

Below we make predictions for the population

```{r}
newdat = data.frame(Days = 0:9)
m3_mean_pp = posterior_linpred(m3, 
                               newdata = newdat, 
                               re_formula = NA) # exclude random effects
```

## Use samples to make predictions

Predictions for the population (based on 1000 samples from the posterior)

```{r, echo=F}
plot(NA, xlim=c(1,10), ylim=c(200,400), axes=F, xlab="Days", ylab="Reaction", main="posterior predictions for average")
axis(1, at=1:10, labels = 0:9); axis(2)
invisible(lapply(sample(4000, size = 1000), 
       function(x) lines(m3_mean_pp[x,], col=rgb(.2,.2,.2,.2))))
```

## Use samples to make predictions

Mean and 95% CI (red points are observed means)

```{r, echo=F}
x = t(apply(m3_mean_pp, 2, FUN = function(x) c(mean=mean(x), quantile(x, probs = c(.025,.975)))))
x=data.frame(x)
colnames(x) = c("mean", "lower", "upper")

plot(NA, xlim=c(1,10), ylim=c(200,400), axes=F, xlab="Days", ylab="Reaction", main="posterior predictions for average")
axis(1, at=1:10, labels = 0:9); axis(2)
with(x, polygon(x = c(1:10, 10:1), y = c(lower, rev(upper)), col = "lightblue", border = NA))
with(x, lines(1:10, mean))

with(aggregate(Reaction~Days, FUN = mean, data = sleepstudy), points(Days+1, Reaction, pch=16, type='b', col='red'))

```

# Warning messages

## Divergent transitions

```r
1: There were 15 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. 
2: Examine the pairs() plot to diagnose sampling problems
```

Could be:

- The steps taken are too big; in which case, increase `adapt_delta`
- The model needs to be reparameterized (in which case the above will not help. More [here](https://mc-stan.org/docs/2_18/stan-users-guide/reparameterization-section.html))


## Divergent transitions

```r
1: There were X divergent transitions after warmup. 
Increasing adapt_delta above 0.8 may help. 
2: Examine the pairs() plot to diagnose sampling problems
```

- `adapt_delta` controls the size of sampling steps, increasing leads to smaller steps
- To increase, add `control = list(adapt_delta = 0.95))` (or another value > .8) to the `brm()` call

## Maximum treedepth exceeded

```r
## Warning: There were X transitions after warmup that exceeded the maximum treedepth.
Increase max_treedepth above 10.
See ## http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded
```

- The No-U Turn Sampler is being stopped too early (to avoid long run time)
- Can be increased via `control = list(max_treedepth = 15))` (or another value > 10)
- Sometimes also indicates that model should be reparameterized (see [here](https://mc-stan.org/docs/2_18/reference-manual/hmc-algorithm-parameters.html))

## Convergence Failure

```r
## Warning: The model has not converged (some Rhats are > 1.1). 
Do not analyse the results! 
## We recommend running more iterations and/or setting stronger priors.
```

For all possible warnings see: [Brief guide to Stan's warnings](https://mc-stan.org/misc/warnings.html#compiler-warnings)

# Examples

## brms-example1.R

- This analysis has been covered in these slides
- Contains some possible extensions to the models we fit at the end
- *Note:* I usually wouldn't throw all of these methods (i.e., bridge sampling, Bayes factors, loo, waic, Savage-Dickey, ROPE, CI, HDI) at one problem. This is just to demonstrate the range of `brms`
- The approach you use will be determined by your goals/ interests (e.g., estimation vs. model comparison vs. null testing)

## brms-example2.R {.columns-2}

```{r, echo=F, fig.height=5, fig.width=5}
dat <- read.csv("../examples/accuracy-data.csv")

dat = within(dat, {
  id = as.factor(id)
  item = as.factor(item)
  condition = as.factor(condition)
  group = as.factor(group)
})

dat_agg = aggregate(acc ~ id + condition + group, data = dat, FUN = mean)

dat_means = aggregate(acc ~ condition + group, data = dat, FUN = mean)

## plot individuals and means...
cols_l = viridis::viridis(2, begin = .2, end = .8, alpha = .5)
cols_d = viridis::viridis(2, begin = .2, end = .8)

ids = list()
ids[[1]] = 1:20
ids[[2]] = 21:40

jitts = c(-.05, .05) # so groups are side-by-side

plot(NA, xlim=c(.7, 2.3), ylim=c(0,1), xlab="Condition", ylab="Accuracy", axes=F)
axis(1, at=1:2, labels = c("A", "B"))
axis(2)

for (g in c(1, 2)){
  # individual data points
  lapply(ids[[g]], function(x) with(subset(dat_agg, id==x), points(jitter(as.numeric(condition)+jitts[g], amount = .025), acc, pch=16, col=cols_l[g], type='p')))
}
for (g in c(1, 2)){
  with(subset(dat_means, group==g), points(as.numeric(condition), acc, pch=21, bg=cols_d[g], type='b'))
}
legend(x = 1.5, y = .2, legend = c("1", "2"), pch = 21, pt.bg = cols_d, title = "Group", xjust = .5, yjust=.5, bty='n')


```

- Hypothetical data from a memory experiment
- Recall accuracy for two groups under two conditions
- The same items appear in both conditions
- `bernoulli(link = "logit")`

## brms-example3.R {.columns-2}

```{r, echo=F, fig.height=5, fig.width=5}
wine <- read.csv("../examples/wine.csv")

wine_means = aggregate(rating ~ contact + temp, data = wine, FUN = mean)

plot(NA, xlim=c(.7, 2.3), ylim=c(.5,5.5), xlab="Temp", ylab="Rating", axes=F)
axis(1, at=1:2, labels = c("Cold", "Warm"))
axis(2)

for (cont in c(1,2)){
  cont_l = c("no", "yes")[cont]
    # individual data points
  lapply(1:9, function(x) with(subset(wine, judge==x & contact==cont_l), points(jitter(as.numeric(temp)+jitts[cont], amount = .025), rating, pch=16, col=cols_l[cont], type='p')))
}

for (cont in c(1,2)){
  cont_l = c("no", "yes")[cont]
  with(subset(wine_means, contact==cont_l), points(as.numeric(temp), rating, pch=21, bg=cols_d[cont], type='b'))
}

legend(x = 1.5, y = 5.5, legend = c("No", "Yes"), pch = 21, pt.bg = cols_d, title = "Contact", xjust = .5, yjust=1, bty='n')

```

- Uses the [`wine` dataset](https://rdrr.io/rforge/ordinal/man/wine.html) from the `ordinal` package
- Temperature and whether there was contact between juice and skin during production
- Rating of bitterness
- `cumulative(link = "probit")`

## brms-example3.R

```{r, echo=F}

curve(dnorm(x), from=-4, to = 4, n = 10000, xlab="", ylab="", axes=F, lwd=2, ylim=c(0, dnorm(0)*1.1))

curve(dnorm(x, mean = .5), from=-4, to = 4, n = 10000, xlab="", ylab="",
      lwd=2, col="violet", add = T)

axis(1)

thresh = seq(-2, 2, length.out = 4)
abline(v = thresh, lty=2, col="red")

text(x = thresh, y = dnorm(0)*1.1, labels = c(as.expression(bquote(tau[1])),
                                          as.expression(bquote(tau[2])),
                                          as.expression(bquote(tau[3])),
                                          as.expression(bquote(tau[4]))
                                          ), adj =-.2)

```

More about the cumulative model [here](https://journals.sagepub.com/doi/abs/10.1177/2515245918823199)

# More model families

## `family`

This workshop covers `gaussian`, `bernoulli/binomial`, and `cumulative` families as they will probably be most used by psychologists. Others that might be useful:

- `von_mises` for circular data
- `multinomial` for multiple categorical outcomes
- `lognormal` for positively skewed distributions (many options)

See `?brmsfamily` for a full range

# Additional functionality | stuff we didn't cover but might be useful to you

## Including known standard errors

`brms` allows to include standard errors for meta-analyses

```r
fit1 <- brm(y | se(sey) ~ x, data = dat)
```

Alternatively, you can do weighted regression

```r
fit1 <- brm(y | weights(wy) ~ x, data = dat)
```

See details in `?brmsformula` for more

## Censored and truncated data

For censored data, you need to specify type of censoring (left, right, interval) and the bound for interval censoring (see `?brmsformula`)

```r
fit1 <- brm(y | cens(censored) ~ x, data = dat)
```

If the outcome has a truncated range, this can be specified:

```r
fit1 <- brm(y | trunc(lb = 0, ub = 100) ~ x, data = dat)
```

## Monotonic effects

From brms-example1.R

- For an ordinal predictor we can treat it as a monotonic predictor where differences between adjacent levels are not assumed to be equal
- Instead `mo()` allows you to model the effect of the predictor as monotonically increasing or decreasing (but not linear)
- see `vignette("brms_monotonic")` for more

```r
m4 = brm(Reaction ~ mo(Days) + (1 + mo(Days) | Subject), data = sleepstudy, 
         family = gaussian, 
         prior=priors
```

## Smooth terms

It is possible to use functions from the [`mgcv` package](https://cran.r-project.org/web/packages/mgcv/index.html) to implement generalized additive models

```r
fit1 <- brm(y ~ s(x), data = dat)
```

See this [blog](https://www.fromthebottomoftheheap.net/2018/04/21/fitting-gams-with-brms/) for more detail

## Distributional regression

From brms-example1.R

- `brms` allows you to model other parameters of the response distribution
- In this case, `sigma` (residual variance) is allowed to vary with Days of sleep deprivation

```r
m5_form = bf(Reaction ~ Days + (1 + Days | Subject), 
             sigma ~ Days)
            
priors = priors +
         set_prior("cauchy(0, 50)", class = "b", dpar = "sigma")

m5 = brm(m5_form, data = sleepstudy, 
         family = gaussian, 
         prior=priors)
```

## Useful Links

- https://cran.r-project.org/web/packages/brms/vignettes/ (includes articles on using `brms`)
- [Signal detection theory](https://vuorre.netlify.com/post/2017/10/09/bayesian-estimation-of-signal-detection-theory-models-part-1/)
- [Ordinal models](https://journals.sagepub.com/doi/abs/10.1177/2515245918823199)
- [Monotonic effects](https://psyarxiv.com/9qkhj/)
- [Bayes factors](https://rpubs.com/lindeloev/bayes_factors)

